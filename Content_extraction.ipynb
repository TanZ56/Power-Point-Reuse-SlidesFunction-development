{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to\n",
      "[nltk_data]     /Users/zeyutan/nltk_data...\n",
      "[nltk_data]   Package conll2002 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/zeyutan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/zeyutan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     /Users/zeyutan/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/zeyutan/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/zeyutan/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/zeyutan/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package ieer to /Users/zeyutan/nltk_data...\n",
      "[nltk_data]   Package ieer is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pptx import Presentation\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "import spacy\n",
    "from nltk.corpus import conll2000\n",
    "from nltk.corpus import conll2002\n",
    "import re # Regular expression operations\n",
    "nltk.download('conll2002')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('conll2000')\n",
    "nltk.download('treebank')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('ieer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f7e0fd2251740b9f10436eaa2b92a7d8.pptx ./output/f7e0fd2251740b9f10436eaa2b92a7d8.pptx\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Step 1: get the raw text from pptx files\n",
    "'''\n",
    "\n",
    "f= open(\"text_extracted_with_newline.txt\",\"w+\")\n",
    "f_raw = open(\"raw_text.txt\",'w+')\n",
    "success = 0\n",
    "fail = 0\n",
    "test_text = ''\n",
    "sentences = []\n",
    "with os.scandir('./output') as it:\n",
    "    for entry in it:\n",
    "        if entry.is_file():\n",
    "            print(entry.name, entry.path)\n",
    "            try:\n",
    "                cur_text = ''\n",
    "                prs = Presentation(entry.path)\n",
    "                for slide in prs.slides:\n",
    "                    for shape in slide.shapes:\n",
    "                        if hasattr(shape, \"text\"):\n",
    "                            sentences.extend(shape.text.split('. '))\n",
    "                            f_raw.write(shape.text)\n",
    "                success += 1\n",
    "            except Exception:\n",
    "                fail += 1\n",
    "                pass\n",
    "        break\n",
    "while(\"\" in sentences) :\n",
    "    sentences.remove(\"\")\n",
    "\n",
    "nonempty_sentences = []\n",
    "for sentence in sentences:\n",
    "    single_str_list = sentence.split(' ')\n",
    "    if len(single_str_list) > 1:\n",
    "        nonempty_sentences.append(sentence)\n",
    "\n",
    "cur_sen = 0\n",
    "# print(len(nonempty_sentences))\n",
    "for sentence in nonempty_sentences:\n",
    "    cur_sen += 1\n",
    "#         sentence.replace('\\n',' ')\n",
    "    if cur_sen == len(nonempty_sentences):\n",
    "        sentence = sentence\n",
    "    else:\n",
    "        sentence = sentence +'\\n'  \n",
    "    f.write(sentence)\n",
    "\n",
    "f.close()\n",
    "\n",
    "line_num = 0\n",
    "f = open(\"text_extracted_without_newline.txt\",\"w+\")\n",
    "with open('text_extracted_with_newline.txt','r') as file:\n",
    "    for line in file:\n",
    "        line_num += 1\n",
    "        if line != '\\n':\n",
    "            f.write(line)\n",
    "f.close()\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "step 2 method collection\n",
    "'''\n",
    "\n",
    "def ie_preprocess(raw_text):\n",
    "    sentences = nltk.sent_tokenize(raw_text)\n",
    "#     print(\"segmentation result: \")\n",
    "#     print(sentences)\n",
    "#     print(\"-----------------------------------------------------\")\n",
    "\n",
    "    sentences = [nltk.word_tokenize(word) for word in sentences]\n",
    "#     print(\"tokennization result: \")\n",
    "#     print_list_of_list(sentences)\n",
    "#     print(\"-----------------------------------------------------\")\n",
    "\n",
    "    sentences = [nltk.pos_tag(word) for word in sentences]\n",
    "#     print(\"part of speach tagging result: \")\n",
    "#     print_list_of_list(sentences)\n",
    "    \n",
    "    return sentences\n",
    "    \n",
    "def print_list(l):\n",
    "    for element in l:\n",
    "        print(element)\n",
    "\n",
    "def print_list_of_list(ll):\n",
    "    for l in ll:\n",
    "        print_list(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segmentation result: \n",
      "['Today is a fine day in Madison.', 'Mr.Tan would like to take a walk.']\n",
      "-----------------------------------------------------\n",
      "tokennization result: \n",
      "Today\n",
      "is\n",
      "a\n",
      "fine\n",
      "day\n",
      "in\n",
      "Madison\n",
      ".\n",
      "Mr.Tan\n",
      "would\n",
      "like\n",
      "to\n",
      "take\n",
      "a\n",
      "walk\n",
      ".\n",
      "-----------------------------------------------------\n",
      "part of speach tagging result: \n",
      "('Today', 'NN')\n",
      "('is', 'VBZ')\n",
      "('a', 'DT')\n",
      "('fine', 'JJ')\n",
      "('day', 'NN')\n",
      "('in', 'IN')\n",
      "('Madison', 'NNP')\n",
      "('.', '.')\n",
      "('Mr.Tan', 'NNP')\n",
      "('would', 'MD')\n",
      "('like', 'VB')\n",
      "('to', 'TO')\n",
      "('take', 'VB')\n",
      "('a', 'DT')\n",
      "('walk', 'NN')\n",
      "('.', '.')\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1, 1.1\n",
    "Step 2: segmentize(seperate str in to sentences),tokenize (seperate sentences into words), \n",
    "pos tag(part-of-speech tag) the raw text\n",
    "'''\n",
    "\n",
    "doc_test1 = \"Today is a fine day in Madison. Mr.Tan would like to take a walk.\"\n",
    "sentences = ie_preprocess(doc_test1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Today/NN)\n",
      "  is/VBZ\n",
      "  (NP a/DT fine/JJ day/NN)\n",
      "  in/IN\n",
      "  Madison/NNP\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "2, 2.1\n",
    "find Noun phrase chunks\n",
    "create grammar -> use grammar to create regular expression parser -> \n",
    "use parser parse sentence with part of speach tagging\n",
    "'''\n",
    "\n",
    "#define a simple grammar with a single regular-expression pattern rule\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\" # an optional determiner + any number of adj + a noun\n",
    "\n",
    "#create a chunk parser\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "\n",
    "#parse the sentence\n",
    "result = cp.parse(sentences[0])\n",
    "print(result)\n",
    "\n",
    "#result.draw() #visually represent the sentence with noun phrase chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n",
      "(S\n",
      "  (NP Today/NN)\n",
      "  is/VBZ\n",
      "  (NP a/DT fine/JJ day/NN)\n",
      "  in/IN\n",
      "  (NP Madison/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "2.2\n",
    "the grammar doesn't have to be regular expression pattern, it can also be tag pattern\n",
    "A tag pattern is a sequence of part-of-speech tags delimited using angle brackets, \n",
    "e.g. <DT>?<JJ>*<NN>. | <DT>?<JJ.*>*<NN.*>+.\n",
    "'''\n",
    "sentence_test1 = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),\n",
    "                  (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
    "#define a simple grammar with a tag pattern rule\n",
    "grammar = \"NP: {<DT>?<JJ.*>*<NN.*>+}\" # any sequence of tokens beginning with an optional determiner, \n",
    "                                      # followed by zero or more adjectives of any type (including relative adjectives like earlier/JJR), \n",
    "                                      # followed by one or more nouns of any type.\n",
    "\n",
    "#create a chunk parser\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "\n",
    "#parse the sentence\n",
    "result = cp.parse(sentence_test1)\n",
    "print(result)\n",
    "result = cp.parse(sentences[0])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2.3 multiple regular expression rules\n",
    "The chunking rules are applied in turn, successively updating the chunk structure.\n",
    "return result when all rules are applied\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  (NP at/IN the/DT cat/NN))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "2.5 use chink to exclude tag: }...{\n",
    "'''\n",
    "grammar = r\"\"\"\n",
    "  NP:\n",
    "    {<.*>+}          # Chunk everything\n",
    "    }<VBD|IN>+{      # Chink sequences of VBD and IN\n",
    "  \"\"\"\n",
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),\n",
    "       (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "print(cp.parse(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2.6\n",
    "represent chunk\n",
    "1. IOB(I:inside the chunk O: outside the chunk B: beginning of the chunk)\n",
    "2. tree\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PP Over/IN)\n",
      "  (NP a/DT cup/NN)\n",
      "  (PP of/IN)\n",
      "  (NP coffee/NN)\n",
      "  ,/,\n",
      "  (NP Mr./NNP Stone/NNP)\n",
      "  (VP told/VBD)\n",
      "  (NP his/PRP$ story/NN)\n",
      "  ./.)\n",
      "(S\n",
      "  Over/IN\n",
      "  (NP a/DT cup/NN)\n",
      "  of/IN\n",
      "  (NP coffee/NN)\n",
      "  ,/,\n",
      "  (NP Mr./NNP Stone/NNP)\n",
      "  told/VBD\n",
      "  (NP his/PRP$ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "3, 3.1\n",
    "Conll2000 text corpus       \n",
    "'''\n",
    "print(conll2000.chunked_sents('train.txt')[99])# conll2000 has tain and test set in pos tag + IOB form\n",
    "\n",
    "print(conll2000.chunked_sents('train.txt', chunk_types=['NP'])[99]) # use chunk parameter to only see np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)] for sent in train_sents]# map each chunk tree to \n",
    "                                                                                                   # a list of word,tag,chunk triples\n",
    "        self.tagger = nltk.UnigramTagger(train_data) # use training data to train a unigram tagger, save the tagger as self.tagger\n",
    "\n",
    "    def parse(self, sentence): # sentence = tagged sentence\n",
    "        # extracting the part-of-speech tags from that sentence\n",
    "        pos_tags = [pos for (word,pos) in sentence] \n",
    "        \n",
    "        # tags the part-of-speech tags with IOB chunk tags, trained in constructor\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags) \n",
    "        \n",
    "        # extracts the chunk tags, and combines them with the original sentence, to yield conlltags. \n",
    "        # unzip the sentence to(word,pos)turple\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags] \n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        \n",
    "        # uses conlltags2tree to convert the result back into a chunk tree\n",
    "        return nltk.chunk.conlltags2tree(conlltags)\n",
    "\n",
    "    \n",
    "    \n",
    "class BigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.BigramTagger(train_data)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word,pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  43.4%%\n",
      "    Precision:      0.0%%\n",
      "    Recall:         0.0%%\n",
      "    F-Measure:      0.0%%\n",
      "----------------------------\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  87.7%%\n",
      "    Precision:     70.6%%\n",
      "    Recall:        67.8%%\n",
      "    F-Measure:     69.2%%\n",
      "----------------------------\n",
      "[[('Today', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('fine', 'JJ'), ('day', 'NN'), ('in', 'IN'), ('Madison', 'NNP'), ('.', '.')], [('Mr.Tan', 'NNP'), ('would', 'MD'), ('like', 'VB'), ('to', 'TO'), ('take', 'VB'), ('a', 'DT'), ('walk', 'NN'), ('.', '.')]]\n",
      "(S\n",
      "  (NP Today/NN)\n",
      "  is/VBZ\n",
      "  (NP a/DT fine/JJ day/NN)\n",
      "  in/IN\n",
      "  (NP Madison/NNP)\n",
      "  ./.)\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.9%%\n",
      "    Precision:     79.9%%\n",
      "    Recall:        86.8%%\n",
      "    F-Measure:     83.2%%\n",
      "----------------------------\n",
      "[('#', 'B-NP'), ('$', 'B-NP'), (\"''\", 'O'), ('(', 'O'), (')', 'O'), (',', 'O'), ('.', 'O'), (':', 'O'), ('CC', 'O'), ('CD', 'I-NP'), ('DT', 'B-NP'), ('EX', 'B-NP'), ('FW', 'I-NP'), ('IN', 'O'), ('JJ', 'I-NP'), ('JJR', 'B-NP'), ('JJS', 'I-NP'), ('MD', 'O'), ('NN', 'I-NP'), ('NNP', 'I-NP'), ('NNPS', 'I-NP'), ('NNS', 'I-NP'), ('PDT', 'B-NP'), ('POS', 'B-NP'), ('PRP', 'B-NP'), ('PRP$', 'B-NP'), ('RB', 'O'), ('RBR', 'O'), ('RBS', 'B-NP'), ('RP', 'O'), ('SYM', 'O'), ('TO', 'O'), ('UH', 'O'), ('VB', 'O'), ('VBD', 'O'), ('VBG', 'O'), ('VBN', 'O'), ('VBP', 'O'), ('VBZ', 'O'), ('WDT', 'B-NP'), ('WP', 'B-NP'), ('WP$', 'B-NP'), ('WRB', 'O'), ('``', 'O')]\n",
      "----------------------------\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  93.3%%\n",
      "    Precision:     82.3%%\n",
      "    Recall:        86.8%%\n",
      "    F-Measure:     84.5%%\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "3.2\n",
    "Simple Evaluation and Baselines\n",
    "'''\n",
    "# make the trivial chunk parser cp that creates no chunks the baseline\n",
    "cp = nltk.RegexpParser(\"\")\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "print(cp.evaluate(test_sents))# 43.4% of words are in the chunk(NP chunk, because chunk_types = [\"NP\"])\n",
    "print(\"----------------------------\")\n",
    "\n",
    "grammar = r\"NP: {<[CDJNP].*>+}\"\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "print(cp.evaluate(test_sents))\n",
    "print(\"----------------------------\")\n",
    "\n",
    "'''unigram tagger is a tagger that only uses a single word as its context for determining the POS(Part-of-Speech) tag.\n",
    "In simple words, Unigram Tagger is a context-based tagger whose context is a single word, i.e., Unigram.'''\n",
    "\n",
    "\n",
    "'''use the training corpus to find the chunk tag (I, O, or B) that is most likely for each part-of-speech tag.\n",
    "e.g determine the correct chunk tag, given each word's part-of-speech tag, not word itself. (j -> B,I)\n",
    "\n",
    "want to create a unigram tagger -> a list of training sentences, which will be in the form of chunk trees ->\n",
    "use nltk.chunk.tree2conlltags(sent) to convert tree to IOB form(map each chunk tree to a list of word,tag,chunk triples) ->\n",
    "\n",
    "'''\n",
    "# train UnigramChunker using the CoNLL 2000 corpus\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n",
    "unigram_chunker = UnigramChunker(train_sents)\n",
    "print(sentences)\n",
    "print(unigram_chunker.parse(sentences[0]))\n",
    "\n",
    "print(unigram_chunker.evaluate(test_sents))\n",
    "print(\"----------------------------\")\n",
    "\n",
    "# each type pos tag represents chunk tag\n",
    "postags = sorted(set(pos for sent in train_sents for (word,pos) in sent.leaves()))\n",
    "print(unigram_chunker.tagger.tag(postags))\n",
    "print(\"----------------------------\")\n",
    "\n",
    "# use Bigramchunker\n",
    "bigram_chunker = BigramChunker(train_sents)\n",
    "print(bigram_chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "3.3\n",
    "1. regular-expression based chunkers\n",
    "2. n-gram chunkers(Bigram or Unigram chunker)\n",
    "1 and 2 chunkers decide what chunks to create entirely based on part-of-speech tags\n",
    "\n",
    "Some sentence we have exactly the same pos tags but are chunked differently:\n",
    "    1> Joey/NN sold/VBD the/DT farmer/NN rice/NN ./.\n",
    "    2> Nick/NN broke/VBD my/DT computer/NN monitor/NN ./.\n",
    "    \n",
    "So pos tag is not sufficient and 1,2 chunkers cannot be used\n",
    "we need to make use of information about the content of the words, \n",
    "in addition to just their part-of-speech tags, if we wish to maximize chunking performance.\n",
    "Classifier-Based Chunkers should be used\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Mary/NN)\n",
      "  saw/VBD\n",
      "  (CLAUSE\n",
      "    (NP the/DT cat/NN)\n",
      "    (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))\n",
      "(S\n",
      "  (NP John/NNP)\n",
      "  thinks/VBZ\n",
      "  (CLAUSE\n",
      "    (NP Mary/NN)\n",
      "    (VP\n",
      "      saw/VBD\n",
      "      (CLAUSE\n",
      "        (NP the/DT cat/NN)\n",
      "        (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "4.1\n",
    "chunk structures have been relatively flat, 1 layer(NP,VP,PP).\n",
    "It is possible to build chunk structures of arbitrary depth, \n",
    "simply by creating a multi-stage chunk grammar containing recursive rules. \n",
    "'''\n",
    "\n",
    "# a four-stage chunk grammar, and can be used to create structures having a depth of at most four.\n",
    "grammar = r\"\"\"\n",
    "  NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN\n",
    "  PP: {<IN><NP>}               # Chunk prepositions followed by NP\n",
    "  VP: {<VB.*><NP|PP|CLAUSE>+$} # Chunk verbs and their arguments\n",
    "  CLAUSE: {<NP><VP>}           # Chunk NP, VP\n",
    "  \"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "sentence = [(\"Mary\", \"NN\"), (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"),\n",
    "    (\"sit\", \"VB\"), (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\n",
    "\n",
    "print(cp.parse(sentence)) # sit on the mat (vp(pp(np))), however, it missed a vp headed by \"saw\"\n",
    "\n",
    "# add an optional second argument loop to specify the number of times the set of patterns should be run\n",
    "# more clauses can be found \n",
    "cp = nltk.RegexpParser(grammar, loop = 2) \n",
    "sentence = [(\"John\", \"NNP\"), (\"thinks\", \"VBZ\"), (\"Mary\", \"NN\"),\n",
    "     (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"), (\"sit\", \"VB\"),\n",
    "     (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\n",
    "print(cp.parse(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP Alice)\n",
      "--------------------------\n",
      "(NP the rabbit)\n",
      "--------------------------\n",
      "(S (NP Alice) (VP chased (NP the rabbit)))\n",
      "--------------------------\n",
      "VP\n",
      "--------------------------\n",
      "['Alice', 'chased', 'the', 'rabbit']\n",
      "--------------------------\n",
      "rabbit\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "4.2\n",
    "A tree is a set of connected labeled nodes, each reachable by a unique path from a distinguished root node. \n",
    "Create trees in nltk\n",
    "'''\n",
    "#In NLTK, we create a tree by giving a node label and a list of children\n",
    "tree1 = nltk.Tree('NP', ['Alice'])\n",
    "print(tree1)\n",
    "print(\"--------------------------\")\n",
    "\n",
    "tree2 = nltk.Tree('NP', ['the', 'rabbit'])\n",
    "print(tree2)\n",
    "print(\"--------------------------\")\n",
    "\n",
    "# incorporate tree 1 and tree 2 into successively larger trees\n",
    "tree3 = nltk.Tree('VP', ['chased', tree2])\n",
    "tree4 = nltk.Tree('S', [tree1, tree3])\n",
    "print(tree4)\n",
    "print(\"--------------------------\")\n",
    "\n",
    "print(tree4[1].label())\n",
    "print(\"--------------------------\")\n",
    "\n",
    "print(tree4.leaves())\n",
    "print(\"--------------------------\")\n",
    "\n",
    "print(tree4[1][1][1]) # s -> vp -> np -> \"rabbit\"\n",
    "\n",
    "# tree4.draw()  # visualize the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  (NE U.S./NNP)\n",
      "  is/VBZ\n",
      "  one/CD\n",
      "  of/IN\n",
      "  the/DT\n",
      "  few/JJ\n",
      "  industrialized/VBN\n",
      "  nations/NNS\n",
      "  that/WDT\n",
      "  *T*-7/-NONE-\n",
      "  does/VBZ\n",
      "  n't/RB\n",
      "  have/VB\n",
      "  a/DT\n",
      "  higher/JJR\n",
      "  standard/NN\n",
      "  of/IN\n",
      "  regulation/NN\n",
      "  for/IN\n",
      "  the/DT\n",
      "  smooth/JJ\n",
      "  ,/,\n",
      "  needle-like/JJ\n",
      "  fibers/NNS\n",
      "  such/JJ\n",
      "  as/IN\n",
      "  crocidolite/NN\n",
      "  that/WDT\n",
      "  *T*-1/-NONE-\n",
      "  are/VBP\n",
      "  classified/VBN\n",
      "  *-5/-NONE-\n",
      "  as/IN\n",
      "  amphobiles/NNS\n",
      "  ,/,\n",
      "  according/VBG\n",
      "  to/TO\n",
      "  (NE Brooke/NNP)\n",
      "  T./NNP\n",
      "  Mossman/NNP\n",
      "  ,/,\n",
      "  a/DT\n",
      "  professor/NN\n",
      "  of/IN\n",
      "  pathlogy/NN\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (NE University/NNP)\n",
      "  of/IN\n",
      "  (NE Vermont/NNP College/NNP)\n",
      "  of/IN\n",
      "  (NE Medicine/NNP)\n",
      "  ./.)\n",
      "(S\n",
      "  The/DT\n",
      "  (GPE U.S./NNP)\n",
      "  is/VBZ\n",
      "  one/CD\n",
      "  of/IN\n",
      "  the/DT\n",
      "  few/JJ\n",
      "  industrialized/VBN\n",
      "  nations/NNS\n",
      "  that/WDT\n",
      "  *T*-7/-NONE-\n",
      "  does/VBZ\n",
      "  n't/RB\n",
      "  have/VB\n",
      "  a/DT\n",
      "  higher/JJR\n",
      "  standard/NN\n",
      "  of/IN\n",
      "  regulation/NN\n",
      "  for/IN\n",
      "  the/DT\n",
      "  smooth/JJ\n",
      "  ,/,\n",
      "  needle-like/JJ\n",
      "  fibers/NNS\n",
      "  such/JJ\n",
      "  as/IN\n",
      "  crocidolite/NN\n",
      "  that/WDT\n",
      "  *T*-1/-NONE-\n",
      "  are/VBP\n",
      "  classified/VBN\n",
      "  *-5/-NONE-\n",
      "  as/IN\n",
      "  amphobiles/NNS\n",
      "  ,/,\n",
      "  according/VBG\n",
      "  to/TO\n",
      "  (PERSON Brooke/NNP T./NNP Mossman/NNP)\n",
      "  ,/,\n",
      "  a/DT\n",
      "  professor/NN\n",
      "  of/IN\n",
      "  pathlogy/NN\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION University/NNP)\n",
      "  of/IN\n",
      "  (PERSON Vermont/NNP College/NNP)\n",
      "  of/IN\n",
      "  (GPE Medicine/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Step 3: entity detection\n",
    "5 Named entity recognition\n",
    "\n",
    "named entities (NEs), definite noun phrases that refer to specific types of individuals,\n",
    "such as organizations, persons, dates, and so on.\n",
    "\n",
    "Goal of a named entity recognition (NER) system is to identify all textual mentions of the named entities.\n",
    "This can be broken down into two sub-tasks: \n",
    "1. identifying the boundaries of the NE \n",
    "2. identifying NE type.\n",
    "\n",
    "'''\n",
    "# NLTK provides a classifier that has already been trained to recognize named entities, \n",
    "# accessed with the function nltk.ne_chunk().\n",
    "sent = nltk.corpus.treebank.tagged_sents()[22] # list of turple of words with pos tag\n",
    "print(nltk.ne_chunk(sent, binary=True))\n",
    "\n",
    "# If we set the parameter binary=True, then named entities are just tagged as NE;\n",
    "# otherwise, the classifier adds category labels such as PERSON, ORGANIZATION, and GPE\n",
    "print(nltk.ne_chunk(sent)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ORG: 'WHYY'] 'in' [LOC: 'Philadelphia']\n",
      "[ORG: 'McGlashan &AMP; Sarrail'] 'firm in' [LOC: 'San Mateo']\n",
      "[ORG: 'Freedom Forum'] 'in' [LOC: 'Arlington']\n",
      "[ORG: 'Brookings Institution'] ', the research group in' [LOC: 'Washington']\n",
      "[ORG: 'Idealab'] ', a self-described business incubator based in' [LOC: 'Los Angeles']\n",
      "[ORG: 'Open Text'] ', based in' [LOC: 'Waterloo']\n",
      "[ORG: 'WGBH'] 'in' [LOC: 'Boston']\n",
      "[ORG: 'Bastille Opera'] 'in' [LOC: 'Paris']\n",
      "[ORG: 'Omnicom'] 'in' [LOC: 'New York']\n",
      "[ORG: 'DDB Needham'] 'in' [LOC: 'New York']\n",
      "[ORG: 'Kaplan Thaler Group'] 'in' [LOC: 'New York']\n",
      "[ORG: 'BBDO South'] 'in' [LOC: 'Atlanta']\n",
      "[ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']\n",
      "-----------------------------------------------\n",
      "VAN(\"cornet_d'elzius\", 'buitenlandse_handel')\n",
      "VAN('johan_rottiers', 'kardinaal_van_roey_instituut')\n",
      "VAN('annie_lennox', 'eurythmics')\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Step 4: relation detection\n",
    "After NEs are recognized, we want to get the relations between NEs\n",
    "we will typically be looking for relations between specified types of named entity.\n",
    "\n",
    "Different method do identifies the turples we want:\n",
    "1. find (a, x, b), where X and Y are named entities ofthe required types, \n",
    "and x is the string of words that intervenes between X and Y. e.g a = PERSON, b = GPE;\n",
    "then use regular expressions to pull out just those instances of α that express the relation that we are looking for.\n",
    "    \n",
    "2. devise patterns that are sensitive to part of speach tags for higher accuracy\n",
    "(some corpus not only provide annotated NE but also provide pos rags)\n",
    "'''\n",
    "# Method 1:\n",
    "IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)') # search for x = string that contains in and b is not a gerund(动名词)\n",
    "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
    "    for rel in nltk.sem.extract_rels('ORG', 'LOC', doc, corpus='ieer', pattern = IN):\n",
    "        print(nltk.sem.rtuple(rel))\n",
    "print(\"-----------------------------------------------\")\n",
    "\n",
    "# method 2:\n",
    "vnv = \"\"\"\n",
    "(\n",
    "is/V|    # 3rd sing present and\n",
    "was/V|   # past forms of the verb zijn ('be')\n",
    "werd/V|  # and also present\n",
    "wordt/V  # past of worden ('become)\n",
    ")\n",
    ".*       # followed by anything\n",
    "van/Prep # followed by van ('of')\n",
    "\"\"\"\n",
    "VAN = re.compile(vnv, re.VERBOSE) # compile the regular expression\n",
    "for doc in conll2002.chunked_sents('ned.train'):\n",
    "    for rel in nltk.sem.extract_rels('PER', 'ORG', doc, corpus='conll2002', pattern=VAN):\n",
    "        print(nltk.sem.clause(rel, relsym=\"VAN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun phrase: \n",
      "['ABFigure S1', 'GC-MS chromatograms', 'ethanol extracts', 'buttons', 'A', 'roots', 'B', 'L. williamsii', 'Figure S2', 'Gene', 'classification', 'the L.', 'transcriptome', 'Unigenes', 'BLASTx matches', 'the Arabidopsis proteins', 'three main GO categories', 'cellular components', 'molecular functions', 'biological processes', 'The left-hand scale', 'the y-axis', 'the percentage', 'unigenes', 'each category', 'The right-hand log scale', 'the y-axis', 'the number', 'unigenes', 'the same category', 'Figure S3', 'The carbon fixation pathway', 'photosynthetic organisms', 'KEGG', 'http', '//www.genome.ad.jp/kegg/', 'The enzymes', 'bold font', 'the EC numbers', 'red', 'those enzymes', 'whose genes', 'the peyote unigene dataset', 'blue', 'enzymes absent', 'Figure S4', 'Glycolysis / Gluconeogenesis biosynthesis', 'the de novo', 'annotation', 'the L.', 'transcriptome', 'This pathway', 'the KEGG Mapper Reconstruct pathway tool', 'http', '//www.genome.ad.jp/kegg/', 'Enzymes', 'the L. williliamsii unigenes data set', 'red-bold font', 'Figure S5', 'Starch', 'sucrose metabolic pathway', 'L.williamsii', 'The pathway', 'the KEGG website', 'http', '//www.genome.ad.jp/kegg/', 'those enzymes', 'red-bold-font reflect proteins', 'the peyote unigene dataset', 'Figure S6', 'L.', 'metabolic networks', 'glucose', 'S7', 'Alignment', 'pyridoxal-dependent decarboxylase', 'domains', 'the deduced amino acid sequences', 'the UN13591', 'UN15671 L. williamsii unigenes', 'Arabidopsis thaliana ATAAS', 'the Papaver somniferum TYDCs', 'line boxes', 'identical residues present', 'all proteins', 'The alignment', 'the phylogenetic tree', 'Figure 3Figure S8', 'A', 'Alignment', 'the PPO1-KFDV C-terminal', 'domain', 'the UN14261 L.', 'unigene', 'Glycine max PPO1', 'XP_003522849.1', 'B', 'Expression analysis', 'the L. williamsii unigene', 'RNA-seq.Figure S9', 'Real-time PCR validation', 'RNA-seq results', 'A', 'Fold-change expression profiles', 'genes', 'real-time PCR', 'each L. williamsii unigene', 'bars', 'mean ± SE', 'n = 3', 'B', 'Correlation plot', 'fold-change values', 'buttons vs.', 'roots', '7', 'genes', 'RNAseq', 'X-axis', 'PCR methodologies', 'Y-axis', 'AB']\n",
      "-------------------------------------------\n",
      "Word frequency rank:\n",
      "('A', 3)\n",
      "('B', 3)\n",
      "('http', 3)\n",
      "('//www.genome.ad.jp/kegg/', 3)\n",
      "('roots', 2)\n",
      "('the L.', 2)\n",
      "('transcriptome', 2)\n",
      "('the y-axis', 2)\n",
      "('unigenes', 2)\n",
      "('those enzymes', 2)\n",
      "('the peyote unigene dataset', 2)\n",
      "('Alignment', 2)\n",
      "('genes', 2)\n",
      "('ABFigure S1', 1)\n",
      "('GC-MS chromatograms', 1)\n",
      "('ethanol extracts', 1)\n",
      "('buttons', 1)\n",
      "('L. williamsii', 1)\n",
      "('Figure S2', 1)\n",
      "('Gene', 1)\n",
      "('classification', 1)\n",
      "('Unigenes', 1)\n",
      "('BLASTx matches', 1)\n",
      "('the Arabidopsis proteins', 1)\n",
      "('three main GO categories', 1)\n",
      "('cellular components', 1)\n",
      "('molecular functions', 1)\n",
      "('biological processes', 1)\n",
      "('The left-hand scale', 1)\n",
      "('the percentage', 1)\n",
      "('each category', 1)\n",
      "('The right-hand log scale', 1)\n",
      "('the number', 1)\n",
      "('the same category', 1)\n",
      "('Figure S3', 1)\n",
      "('The carbon fixation pathway', 1)\n",
      "('photosynthetic organisms', 1)\n",
      "('KEGG', 1)\n",
      "('The enzymes', 1)\n",
      "('bold font', 1)\n",
      "('the EC numbers', 1)\n",
      "('red', 1)\n",
      "('whose genes', 1)\n",
      "('blue', 1)\n",
      "('enzymes absent', 1)\n",
      "('Figure S4', 1)\n",
      "('Glycolysis / Gluconeogenesis biosynthesis', 1)\n",
      "('the de novo', 1)\n",
      "('annotation', 1)\n",
      "('This pathway', 1)\n",
      "('the KEGG Mapper Reconstruct pathway tool', 1)\n",
      "('Enzymes', 1)\n",
      "('the L. williliamsii unigenes data set', 1)\n",
      "('red-bold font', 1)\n",
      "('Figure S5', 1)\n",
      "('Starch', 1)\n",
      "('sucrose metabolic pathway', 1)\n",
      "('L.williamsii', 1)\n",
      "('The pathway', 1)\n",
      "('the KEGG website', 1)\n",
      "('red-bold-font reflect proteins', 1)\n",
      "('Figure S6', 1)\n",
      "('L.', 1)\n",
      "('metabolic networks', 1)\n",
      "('glucose', 1)\n",
      "('S7', 1)\n",
      "('pyridoxal-dependent decarboxylase', 1)\n",
      "('domains', 1)\n",
      "('the deduced amino acid sequences', 1)\n",
      "('the UN13591', 1)\n",
      "('UN15671 L. williamsii unigenes', 1)\n",
      "('Arabidopsis thaliana ATAAS', 1)\n",
      "('the Papaver somniferum TYDCs', 1)\n",
      "('line boxes', 1)\n",
      "('identical residues present', 1)\n",
      "('all proteins', 1)\n",
      "('The alignment', 1)\n",
      "('the phylogenetic tree', 1)\n",
      "('Figure 3Figure S8', 1)\n",
      "('the PPO1-KFDV C-terminal', 1)\n",
      "('domain', 1)\n",
      "('the UN14261 L.', 1)\n",
      "('unigene', 1)\n",
      "('Glycine max PPO1', 1)\n",
      "('XP_003522849.1', 1)\n",
      "('Expression analysis', 1)\n",
      "('the L. williamsii unigene', 1)\n",
      "('RNA-seq.Figure S9', 1)\n",
      "('Real-time PCR validation', 1)\n",
      "('RNA-seq results', 1)\n",
      "('Fold-change expression profiles', 1)\n",
      "('real-time PCR', 1)\n",
      "('each L. williamsii unigene', 1)\n",
      "('bars', 1)\n",
      "('mean ± SE', 1)\n",
      "('n = 3', 1)\n",
      "('Correlation plot', 1)\n",
      "('fold-change values', 1)\n",
      "('buttons vs.', 1)\n",
      "('7', 1)\n",
      "('RNAseq', 1)\n",
      "('X-axis', 1)\n",
      "('PCR methodologies', 1)\n",
      "('Y-axis', 1)\n",
      "('AB', 1)\n"
     ]
    }
   ],
   "source": [
    "with open('raw_text.txt','r') as file:\n",
    "    ttl_noun_phrase = []\n",
    "    for line in file:\n",
    "        #get the raw text from a file\n",
    "        \n",
    "        # segmentize, tokenize, and pos tag the token\n",
    "        sentences = ie_preprocess(line)\n",
    "        train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n",
    "        unigram_chunker = UnigramChunker(train_sents)\n",
    "        for sentence in sentences:\n",
    "            parsed_cur_sentence = unigram_chunker.parse(sentence)\n",
    "            noun_phrases_cur = [' '.join(leaf[0] for leaf in tree.leaves()) \n",
    "                      for tree in parsed_cur_sentence.subtrees() \n",
    "                      if tree.label()=='NP'] \n",
    "            ttl_noun_phrase.extend(noun_phrases_cur)         \n",
    "    print(\"noun phrase: \")    \n",
    "    print(ttl_noun_phrase)\n",
    "    word_count = {}\n",
    "    for noun in ttl_noun_phrase:\n",
    "        if noun in word_count:\n",
    "            word_count[noun] += 1\n",
    "        else:\n",
    "            word_count[noun] = 1\n",
    "    \n",
    "    print(\"-------------------------------------------\")\n",
    "    print(\"Word frequency rank:\")\n",
    "    \n",
    "    word_frequency = list(sorted(word_count.items(), key=lambda item: item[1],reverse = True))\n",
    "    for rank in word_frequency:\n",
    "        print(rank)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
